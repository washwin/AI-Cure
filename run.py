# -*- coding: utf-8 -*-
"""AI Cure.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lctzYzWm3bH-4dnDa3HMDpd3qhWOUxOd
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

data = pd.read_csv('/content/train_data.csv')
data = data.drop(columns = ['uuid','datasetId'])
data.head()
data = data.dropna()
X = data.drop(columns=['HR'])
y = data['HR']
X_train,X_test,y_train,y_test= train_test_split(X,y)

print(data.dtypes)
lbl = LabelEncoder()
X_train['condition']= lbl.fit_transform(X_train['condition'])
X_test['condition'] =lbl.transform(X_test['condition'])
X_train['condition'].unique()
sns.heatmap(data.corr())

"""All the columns are numerical. Use Pearsson's correlation"""



"""Step 1: Exploratory Data Analysis

Check the null values: No Null Values
"""

print(data['HR'].describe())
px.histogram(data['HR'], title="Distribution of Heart Rate")

from sklearn.feature_selection import f_regression,SelectKBest
def select_features(X_train,y_train,X_test,k=10):
  fs = SelectKBest(score_func=f_regression,k=k)
  print(X_train.isnull().sum(), y_train.isnull().sum())
  # X_train = X_train.dropna(axis=0)
  # y_train = y_train.dropna(axis=0)
  fs.fit(X_train,y_train)
  X_train_fs = fs.fit_transform(X_train,y_train)
  X_test_fs  = fs.transform(X_test)
  return X_train_fs,X_test_fs,fs


X_train_fs,X_test_fs,fs = select_features(X_train,y_train,X_test,k=15)
fig = px.bar(x=list(fs.scores_),y=np.array(X_train.columns),
             labels=dict(x="Feature Score",y="Column Names"))

fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')
fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')
fig.show()

"""Checking the target value distribution"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

def Regression_Model(X_train,X_test,y_train,y_test):
  lr = LinearRegression()
  lr.fit(X_train, y_train)

  train_predict = lr.predict(X_train)
  test_predict  = lr.predict(X_test)

  train_error = mean_squared_error(y_train,train_predict)
  test_error  = mean_squared_error(y_test,test_predict)

  train_score = r2_score(y_train,train_predict)
  test_score  = r2_score(y_test,test_predict)

  print(f"\nEvaluation with {X_train.shape[1]} Features")
  print(f"Test Error = {test_error} Train Error={train_error}")
  print(f"Train Score = {train_score} Test Score={test_score}")


model1 = Regression_Model(X_train_fs,X_test_fs,y_train,y_test)
model2 = Regression_Model(X_train,X_test,y_train,y_test)

"""Using Ensemble Learning"""

from sklearn.ensemble import GradientBoostingRegressor
def GradientBoost_Model(X_train,X_test,y_train,y_test):
  gb = GradientBoostingRegressor(loss='squared_error',learning_rate=0.1,n_estimators=100,min_samples_split=5,max_depth=5)
  gb.fit(X_train, y_train)

  train_predict = gb.predict(X_train)
  test_predict  = gb.predict(X_test)

  train_error = mean_squared_error(y_train,train_predict)
  test_error  = mean_squared_error(y_test,test_predict)

  train_score = r2_score(y_train,train_predict)
  test_score  = r2_score(y_test,test_predict)

  print(f"\nEvaluation with {X_train.shape[1]} Features")
  print(f"Test Error = {test_error} Train Error={train_error}")
  print(f"Train Score = {train_score} Test Score={test_score}")

model1 = GradientBoost_Model(X_train_fs,X_test_fs,y_train,y_test)
model2 = GradientBoost_Model(X_train,X_test,y_train,y_test)